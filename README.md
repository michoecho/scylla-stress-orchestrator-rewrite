# An experimental rewrite of scylla-stress-orchestrator

## Dependencies
terraform, ansible, pyyaml, java, rsync, openssh

## Warnings
- All commands in this repository have to be run from the repository's root directory!
- Do not use any names with whitespaces, the scripts aren't prepared for that and will fail in an untested way.
- Do not modify `prov/ec2*` and the deployment directory while a deployment is active. Terraform state is stored there. Tampering with it can result in leftover VMs.
- Do not move the repository while a deployment is active.
- Running the setup scripts can take some time. In particular, on Scylla's AMI swap setup and `scylla_io_setup` usually take ~2 minutes each. Only become suspicious if they scripts are running for more than 5 minutes.

## Main differences w.r.t. scylla-stress-orchestrator
- Benchmarking scripts are written in async python, instead of custom thread-based futures. I think the outcome is much cleaner. Thanks to using built-in features, it makes parallelizing, awaiting and cancelling futures clean and easy.
- The provisioning script generates an Ansible inventory and a ssh config file from Terraform's output. All further access to VMs is done by hostnames and those config files. This simplifies scripts and helps with interactive work.
- Prometheus metrics are downloaded and cleared using the snapshot API, instead of stopping monitoring, downloading data and restarting monitoring. Restarting monitoring is slow.
- Configuration scripts are written in Ansible, rather than in ssh calls from python and terraform. (Not a very important difference. I'm not sure which approach is the better one yet.)
- There are some helper scripts in bin/ to help with interactive work.
- HDR log processing is parallelized.
- Instead of passing the IPs of everything around separately, there is a central Deployment object in benchmarking scripts (based on data in the Ansible inventory), which holds the IPs and names of all machines.

## Example usage

```
# Create a pair of SSH keys. They will become keys to the AWS VM's.
# Alternatively the user can specify other PEM key paths in credentials.yml.
bin/make-key

# Fill in the AWS credentials which will be used to setup the deployment.
# The name "prov/credentials.yml" isn't special. This is simply a YAML config file which will
# be passed explicitly to bin/provision-terraform.
cp -n prov/credentials.yml.template prov/credentials.yml && vim prov/credentials.yml

# Provision the servers, clients and monitoring VMs on AWS and generate configs to test_deployment/
# The name "test_deployment" will be later used to run any script on this deployment.
#
# There are 2 terraform profiles at the moment: ec2 and ec2-spot.
# The only difference is this: ec2-spot will request spot instances, ec2 will request on-demand instances.
# Clients are spot in both profiles, monitoring is on-demand in both profiles.
#
# The *.yml files contain the configuration for the deployment. The *.yml files and their contents are unordered and
# simply glued into one large config before being processed.
# The necessary config entries are undocumented. Just modify the files from the example below as needed.
prov/provision-terraform test_deployment prov/ec2-spot prov/credentials.yml prov/config-example/*.yml

# Install software and correct default configs on the provisioned VMs. (Here parallelized with GNU parallel for convenience).
# Only needs to be run once. (But nothing will break if it's run multiple times).
parallel --lb bin/ansible-playbook test_deployment setup/configure_{}.playbook ::: servers clients monitoring

# Start Scylla and monitoring.
# If they are running, they will first be killed and their data (but not config) will be removed.
parallel --lb bin/ansible-playbook test_deployment setup/reset_{}.playbook ::: servers monitoring

# Run an example benchmark. It will put the output in trials/test_deployment/$STARTING_TIMEPOINT_IN_SOME_FORMAT/.
python bench/benchmark_latency_throughput.py test_deployment bench/benchmark_latency_throughput_config.yml

# Do an interactive ssh session on server-0.
bin/ssh test_deployment server-0

# Open the live dashboards in firefox.
bin/firefox-monitoring test_deployment

# Destroy all created AWS resources and remove test_deployment/
prov/unprovision-terraform test_deployment
```

## Source reading guide:
- The provisioning part, the setup/configuration part and the benchmark part are separate and coupled only by the configs generated by terraform.py (Ansible inventory and ssh config).
- The provisioning part is contained in `prov/`. ec2 and ec2-spot store Terraform profiles, while terraform.py (and it's wrapper scripts) run Terraform and generate configs based on its output.
- The setup/configuration scripts (written in Ansible) is contained in `setup/`. `*.playbook` and `install_utils.yml` contain the scripts (written in Ansible), `*.j2` are config file templates for Scylla Monitoring, while other `*.yml` files contain example deployment configs.
- The benchmarking part is in `bench/`. `hdr.py` and `utils.py` provide a "framework", while `benchmark.py` contains an example benchmark.
- lib/ contains binary dependencies (right now, only for hdr.py)
- bin/ contains wrapper scripts for ansible and ssh, which translate the deployment name to the relevant config. Some parts depend on them, but they are also helpful for interactive work.
